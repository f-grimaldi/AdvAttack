{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Zero-order Stochastic Conditional Gradient (ZSCG) with Inexact Conditional Gradient (ICG) update.\n",
    "Based on the paper:\n",
    "    Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints, High-Dimensionality and Saddle-Points∗\n",
    "by:\n",
    "    Krishnakumar Balasubramanian†1 and Saeed Ghadimi‡2\n",
    "ALG.4 (modified version for non-convex problem)\n",
    "\"\"\"\n",
    "class InexactZSCG(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Name            Type                Description\n",
    "    model:          (nn.Module)         The model to use to get the output\n",
    "    loss:           (nn.Module)         The loss to minimize\n",
    "    device:\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss, device=torch.device('cuda')):\n",
    "        self.device = device\n",
    "        self.loss = loss\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Perform an attack against the model given an input and the required run params\n",
    "    \"\"\"\n",
    "    def run(self, x, v, n_gradient, gamma_k, mu_k, epsilon, L_type = -1, batch_size = -1, C = (0, 1),\n",
    "            max_steps=100, verbose=0, additional_out=False, tqdm_disabled=False, max_t=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v               (float)             The gaussian smoothing\n",
    "        n_gradient      (list)              Number of normal vector to generate at every step\n",
    "        gamma_k         (list)              Momentum at every step inside ICG\n",
    "        mu_k            (list)              Stoppinc criterion at every step k inside ICG\n",
    "        max_t           (int)               The maximum number of iteration inside of ICG.\n",
    "        epsilon         (float)             The upper bound of norm\n",
    "        L_type          (int)               Either -1 for L_infinity or x for Lx. Default is -1\n",
    "        batch_size      (int)               Maximum parallelization during the gradient estimation. Default is -1 (=n_grad)\n",
    "        C               (tuple)             The boundaires of the pixel. Default is (0, 1)\n",
    "        max_steps       (int)               The maximum number of steps. Default is 100\n",
    "        verbose         (int)               Display information or not. Default is 0\n",
    "        additional_out  (bool)              Return also all the x. Default is False\n",
    "        tqdm_disable    (bool)              Disable the tqdm bar. Default is False\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # 1. Init class attributes\n",
    "        self.x_original = x.clone()\n",
    "        self.dim = x.shape\n",
    "        self.total_dim = torch.prod(torch.tensor(x.shape))\n",
    "        self.epsilon = epsilon\n",
    "        self.L_type = L_type\n",
    "        self.C = C\n",
    "        self.batch = batch_size\n",
    "        self.max_t = max_t\n",
    "\n",
    "        # 2. Init list of results\n",
    "        losses, outs = [], []\n",
    "        x_list = []\n",
    "\n",
    "        # 3. Main optimization cycle\n",
    "        for ep in tqdm(range(max_steps), disable=tqdm_disabled):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"---------------\")\n",
    "                print(\"Step number: {}\".format(ep))\n",
    "\n",
    "            # 3.1 Call the step\n",
    "            x, gk = self.step(x, v, gamma_k[ep], mu_k[ep], n_gradient[ep], verbose)\n",
    "            x = x.reshape(self.dim[0], self.dim[1], self.dim[2]).detach()\n",
    "\n",
    "            # 3.2 Compute loss\n",
    "            out = self.model(x.view(1, self.dim[0], self.dim[1], self.dim[2]))\n",
    "            loss = self.loss(out).view(-1, 1)\n",
    "\n",
    "            # 3.3 Save results\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            outs.append(out.detach().cpu()[0, self.loss.neuron].item())\n",
    "            if additional_out:\n",
    "                x_list.append(x.cpu())\n",
    "\n",
    "            # 3.4 Display current info\n",
    "            if verbose:\n",
    "                print(\"Loss:        {}\".format(losses[-1]))\n",
    "                print(\"Output:      {}\".format(outs[-1]))\n",
    "\n",
    "            # 3.5 Check Stopping criterion\n",
    "            condition1 = (int(torch.argmax(out)) != self.loss.neuron) and (self.loss.maximise == 0)\n",
    "            condition2 = (int(torch.argmax(out)) == self.loss.neuron) and (self.loss.maximise == 1)\n",
    "            if condition1 or condition2:\n",
    "                break\n",
    "\n",
    "        if additional_out:\n",
    "            return x, losses, outs, input_list\n",
    "        return  x, losses, outs\n",
    "\n",
    "    \"\"\"\n",
    "    Do an optimization step\n",
    "    \"\"\"\n",
    "    def step(self, x, v, gamma, mu, mk, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v:              (float)             The gaussian smoothing\n",
    "        gamma:          (float)             The update parameters of g\n",
    "        mu:             (float)             The stopping criterion\n",
    "        mk:             (int)               The number of Gaussian Random Vector to generate\n",
    "        verbose:        (bool)              Display information or not. Default is 0\n",
    "        \"\"\"\n",
    "        # Compute the approximated gradient\n",
    "        g = self.compute_Gk(x, v, mk, verbose)\n",
    "        # Call the inexact conditional gradient\n",
    "        x_new = self.compute_ICG(x, g, gamma, mu, verbose).reshape(x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "        if verbose > 1:\n",
    "            print(\"\\nINSIDE STEP\")\n",
    "            print(\"Gradient has shape: {}\".format(g.shape))\n",
    "            print(\"Gradient is:\\n{}\".format(g))\n",
    "            print(\"x_new has shape: {}\".format(x_new.shape))\n",
    "            print(\"x_new is:\\n{}\".format(x_new))\n",
    "\n",
    "        return x_new.detach(), g.detach()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare parallelization for the gradient estimation\n",
    "    \"\"\"\n",
    "    def get_parallel(self, x, bs, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The current variable\n",
    "        bs              (int)               The maximum bacth size\n",
    "        v               (int)               The Gaussian smoothing\n",
    "        \"\"\"\n",
    "        uk     = torch.empty(bs, self.total_dim).normal_(mean=0, std=1).to(self.device) # Dim (bs, channel*width*height)\n",
    "        img_u  = uk.reshape(bs, self.dim[0], self.dim[1], self.dim[2])                  # Dim (bs, channel, width, height)\n",
    "        img_x  = x.expand(bs, self.dim[0], self.dim[1], self.dim[2])                    # Dim (bs, channel, width, height)\n",
    "        m_x    = (img_x + v*img_u)\n",
    "\n",
    "        return m_x, uk\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Gv(x(k-1), chi(k-1), u(k)) in order to compute an approximation of the gradient of f(x(k-1), chi(k-1))\n",
    "    \"\"\"\n",
    "    def compute_Gk(self, x, v, mk, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v:              (float)             The gaussian smoothing\n",
    "        mk:             (int)               The number of Gaussian Random Vector to generate\n",
    "        verbose:        (bool)              Display information or not. Default is 0\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Get objective functions\n",
    "        # CASE BATCH_SIZE == N_GRADIENT\n",
    "        if self.batch == -1:\n",
    "\n",
    "            # 1.a Compute standard Loss\n",
    "            standard_loss = self.loss(self.model(x.view(1, *list(self.dim))))\n",
    "\n",
    "            # 1.b Compute gaussian loss\n",
    "            m_x, uk = self.get_parallel(x, mk, v)\n",
    "            gaussian_loss = self.loss(self.model(m_x))\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('\\nINSIDE GRADIENT')\n",
    "                print('The Gaussian vector uk has shape:{}'.format(uk.shape))\n",
    "                print('The input x has shape:\\t\\t{}'.format(x.shape))\n",
    "                print('The input x + vu has shape:\\t{}'.format(m_x.shape))\n",
    "\n",
    "            # 1.c Compute Gv(x(k-1), chi(k-1), u(k))\n",
    "            fv = ((gaussian_loss - standard_loss.expand(uk.shape[0]))/v).view(-1, 1)        # Dim (mk, 1)\n",
    "            G = fv * uk                                                                     # Dim (mk, channel*width*height)\n",
    "\n",
    "            return torch.mean(G, axis=0).detach()\n",
    "\n",
    "        # CASE BATCH_SIZE < N_GRADIENT\n",
    "        else:\n",
    "\n",
    "            # 1.a Compute standard loss\n",
    "            standard_loss = self.loss(self.model(x.view(1, *list(self.dim))))                   # Dim (1)\n",
    "            G_tot = torch.zeros(mk//self.batch, self.total_dim).to(self.device)                 # Dim (n_batches, hannel*width*height)\n",
    "\n",
    "            #1.b Compute Gradient\n",
    "            for n in range(mk//self.batch):\n",
    "                from_, to_ = n*self.batch, (n+1)*self.batch\n",
    "\n",
    "                # 1.b Create batch x(k-1) + v*u(k-1)\n",
    "                m_x, uk = self.get_parallel(x, self.batch, v)\n",
    "\n",
    "                # 1.c Compute\n",
    "                tmp_gaussian_loss = self.loss(self.model(m_x)).detach()                                 # Dim(bs)\n",
    "\n",
    "                if verbose > 1:\n",
    "                    print('\\nINSIDE GRADIENT')\n",
    "                    print('The Gaussian vector uk has shape:{}'.format(uk.shape))\n",
    "                    print('The input x has shape:\\t\\t{}'.format(x.shape))\n",
    "                    print('The input x + vu has shape:\\t{}'.format(m_x.shape))\n",
    "\n",
    "                # 1.d Compute Gradient\n",
    "                fv = ((tmp_gaussian_loss - standard_loss.expand(uk.shape[0]))/v).view(-1, 1)            # Dim (bs, 1)\n",
    "                G = fv * uk                                                                             # Dim (bs, channel*width*height)\n",
    "\n",
    "                if verbose > 1:\n",
    "                    print('Gaussian cycle loss has shape:\\t{}'.format(tmp_gaussian_loss.shape))\n",
    "                    print('Function approx has shape:\\t{}'.format(fv.shape))\n",
    "                    print('Gradient has shape:\\t\\t{}'.format(G.shape))\n",
    "\n",
    "                G_tot[n] = torch.mean(G, axis=0).detach()\n",
    "\n",
    "        return torch.mean(G_tot, axis=0).detach()\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Inexact Condtion Gradient (Algorithm 3 of source article)\n",
    "    \"\"\"\n",
    "    def compute_ICG(self, x, g, gamma, mu, verbose):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        g:              (torch.tensor)      The approximated gradient. Should be a 1D tensor\n",
    "        gamma:          (float)             The update parameters of g\n",
    "        mu:             (float)             The stopping criterion\n",
    "        \"\"\"\n",
    "        # 1. Init variables\n",
    "        y_old = x.view(-1).clone() # dim = (n_channel * width * height)\n",
    "        t = 1\n",
    "        k = 0\n",
    "        # 2. Main cycle\n",
    "        while(k==0):\n",
    "            # 2.1 Compute gradient\n",
    "            grad = g + gamma*(y_old - x.view(-1))\n",
    "\n",
    "            # 2.2 Perform LMO\n",
    "            # Infinity norm\n",
    "            if self.L_type == -1:\n",
    "                y_new = self.x_original.view(-1) - self.epsilon*torch.sign(g)\n",
    "            # L1 norm\n",
    "            elif self.L_type == 1:\n",
    "                raise NotImplementedError\n",
    "            elif self.L_type == 2:\n",
    "                y_new = self.x_original.view(-1) - (self.epsilon*g)/torch.norm(g, 2)\n",
    "            # Generic Lp norm (1 < p < +inf)\n",
    "            else:\n",
    "                p = self.L_type\n",
    "                gp = torch.abs(g)**(1/p-1)\n",
    "                h = torch.sign(g) * (gp) / torch.norm(gp, p)\n",
    "                y_new = self.x_original.view(-1) - self.epsilon*h\n",
    "\n",
    "            # 2.3 Compute new function value\n",
    "            h = torch.dot(grad, y_new - y_old)\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('\\nINSIDE ICG')\n",
    "                print('Difference between g and y_old -x: {}')\n",
    "                print(torch.norm(g/torch.norm(g) - (y_old - x.view(-1))/torch.norm(y_old - x.view(-1))))\n",
    "                print('Time t = {}'.format(t))\n",
    "                print('The original gradient is:\\n{}'.format(g))\n",
    "                print('The ICG gradient is:\\n{}'.format(grad))\n",
    "                print('The new y is:\\n {}'.format(y_new))\n",
    "                print('The function h(y_new) is {}'.format(h))\n",
    "                print('Mu is: {}'.format(mu))\n",
    "\n",
    "            # 2.4 Check conditions\n",
    "            if h >= -mu or t > self.max_t:\n",
    "                k = 1\n",
    "            else:\n",
    "                y_old = (t-1)/(t+1)*y_old + 2/(t+1)*y_new\n",
    "                t += 1\n",
    "\n",
    "        return self.project_boundaries(y_old.detach())\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Check the boundaries of our constraint optimization problem\n",
    "    \"\"\"\n",
    "    def project_boundaries(self, x):\n",
    "        x[x > self.C[1]] = self.C[1]\n",
    "        x[x < self.C[0]] = self.C[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"\n",
    "Abstract object for the Custom Loss. Child of nn.Module\n",
    "\"\"\"\n",
    "class Loss(object):\n",
    "\n",
    "    def __init__(self, neuron, maximise=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Name       Type    Desc\n",
    "            neuron     int     The output neuron to minimize\n",
    "            maximise   bool    The desired activation\n",
    "        \"\"\"\n",
    "        self.neuron = neuron\n",
    "        self.maximise = maximise\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a target neuron and a target (y_true).\n",
    "Compute the Mean Squared Difference between the softmax output and the target\n",
    "\"\"\"\n",
    "class MSELoss(Loss):\n",
    "\n",
    "    def __init__(self, neuron, maximise=0, is_softmax=False, dim=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Name        Type    Desc\n",
    "            neuron:     int     The output neuron to minimize\n",
    "            maximise    bool.   The desired activation (0/1)\n",
    "            is_softmax:  bool     Bool indicating if the model output is probability distribution. Defaulti is False\n",
    "            dim:         int      Dimension of softmax application. Default is 1\n",
    "        \"\"\"\n",
    "        super().__init__(neuron, maximise)\n",
    "        self.is_softmax = is_softmax\n",
    "        self.dim = dim\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the MSE after computing the softmax of input.\n",
    "    Forward is implemented in the __call__ method of super\n",
    "    \"\"\"\n",
    "    def forward(self, y_pred):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            y_pred  torch.tensor The output of the network. Preferable shape (n_batch, n_classes)\n",
    "        \"\"\"\n",
    "        # Deal with 1D input\n",
    "        if len(y_pred.shape) == 1:\n",
    "            y_pred = y_pred.view(-1, 1)\n",
    "        # Case model output does not have non-softmax model output\n",
    "        if not self.is_softmax:\n",
    "            # Compute logits\n",
    "            y_pred = nn.Softmax(dim=self.dim)(y_pred)\n",
    "        # Return loss\n",
    "        return 0.5*(int(self.maximise) - y_pred[:, self.neuron])**2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() got multiple values for argument 'v'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bc31ca2369bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m x, loss_curve, out, xs = optim.run(x.view(1, 1, 1),\n\u001b[0m\u001b[1;32m     29\u001b[0m                                     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                    \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: run() got multiple values for argument 'v'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 3, (2, 2), stride=2, padding=1)\n",
    "        self.linear = nn.Linear(3, 3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv(x))\n",
    "        return nn.Sigmoid()(self.linear(x.view(x.shape[0], -1)))\n",
    "\n",
    "epoch = 50\n",
    "m = [50]*epoch\n",
    "a = [0.9]*epoch\n",
    "v = 1\n",
    "\n",
    "\n",
    "net = Net()\n",
    "loss = MSELoss(neuron=2, maximise=0)\n",
    "optim = InexactZSCG(model=net, loss=loss)\n",
    "\n",
    "x = torch.tensor([1])\n",
    "x, loss_curve, out, xs = optim.run(x.view(1, 1, 1),\n",
    "                                    m, a,\n",
    "                                   v=0.01,\n",
    "                                   n_gradient=5,\n",
    "                                   gamma_k=2,\n",
    "                                   epsilon=0.5,\n",
    "                                   max_steps=epoch,\n",
    "                                   verbose=0, additional_out=True)\n",
    "\n",
    "min_, max_ = min(xs), max(xs)\n",
    "losses = []\n",
    "for i in tqdm(range(int(min_-1)*10, int(max_+1)*10)):\n",
    "    x = torch.tensor([i/10]).to(torch.device('cuda'))\n",
    "    out = net(x.view(1, 1, 1, 1))\n",
    "    losses.append(loss(out))\n",
    "\n",
    "plt.plot([i/10 for i in range(int(min_-1)*10, int(max_+1)*10)], losses, label='Loss curve')\n",
    "plt.scatter(xs, loss_curve, label='Parameters')\n",
    "plt.legend()\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss function')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
