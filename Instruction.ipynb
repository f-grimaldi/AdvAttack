{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer instruction\n",
    "\n",
    "#### COMMON PARAMETERS\n",
    "\n",
    "All the optimizer class must have in common the following points\n",
    "\n",
    "**1. The initializer of the optimizer must take the following args:**\n",
    "\n",
    "\n",
    "    Args:\n",
    "    Name            Type                Description\n",
    "    model           torch.nn.Module     The model to attack\n",
    "    loss            src.Loss.Loss       The loss to use\n",
    "    device           torch.device        The device used for the computation\n",
    "    \n",
    "    Code example:\n",
    "```\n",
    "device = torch.device('cuda')\n",
    "myNet    = myCustomNetwork()\n",
    "myLoss = myCustomLoss()\n",
    "myOptimizer = myOptimizerClass(model = myNet, loss = myLoss, device = device)\n",
    "```\n",
    "    \n",
    "    \n",
    "**2. The optimizer must have the run method with the following args:**\n",
    "\n",
    "\n",
    "    Args:\n",
    "    Name            Type                Description\n",
    "    x:              (torch.tensor)      The variable of our optimization problem. Must be 3D tensor (img)\n",
    "    n_gradient:     (int)               The number of function evaluation to do to perform the gradient estimation \n",
    "    batch_size:     (int)               The maximum parallelization duting the gradient estimation. Default is -1 (=n_grad) \n",
    "    C:              (tuple)             The boundaires of the pixel. Default is (0, 1)\n",
    "    max_steps:      (int)               The maximum number of steps. Default is 100\n",
    "    verbose:        (int)               Display information or not. Default is 0\n",
    "    additional_out  (bool)              Return also all the x found during the process. Default is False\n",
    "    tqdm_disable    (bool)              Disable the tqdm bar. Default is False\n",
    "\n",
    "    Other common args:\n",
    "    Name            Type                Description\n",
    "    epsilon:        (float)             The upper bound of the norm\n",
    "    L_type:         (int)               Either -1 for L_infinity or Lx for Lx. Default is -1\n",
    "    \n",
    "    Code example:\n",
    "    args              = {'x': my_chosen_tensor_3d_img, 'n_gradient': 3000, 'batch_size' = 100, ...}\n",
    "    other_common_args = {'epsilon': 0.3, 'L_type': -1}\n",
    "    specific_args     = {'beta': 0.2, 'gamma': 0.3}\n",
    "    myOptimizer.run(**args, **other_common_args, **specific_args)\n",
    "      \n",
    "\n",
    "**3. The optimizer must return the following output:**\n",
    "\n",
    "\n",
    "    Outputs\n",
    "    Name            Type                Description\n",
    "    x:              (torch.tensor)      The last found optimization variable. Must be 3D tensor (img)\n",
    "    losses:         (list)              A list (of float) of the losses at each step\n",
    "    outs:           (list)              A list (of float) of the outputs of the target neuron at each step\n",
    "    \n",
    "    Only if additional_out = True\n",
    "    list_of_x       (list)              A list of torch.tensor of the input at each step\n",
    "    \n",
    "    Code example:\n",
    "    new_x, loss_list, out_list = myOptimizer.run(**args, **other_common_args, **specific_args)\n",
    "\n",
    "    \n",
    "**4. Stopping criterion:** <br>\n",
    "\n",
    "    4.1 If the attack is untargeted and the prior label of the original example is n, STOP when the argmax of the model \n",
    "        output of the last found example is different from n \n",
    "    4.2 If the attack is targeted with target label m and the original example has label n, STOP when the argamax of \n",
    "        the model output of the last found example is m\n",
    "        \n",
    "    Code example:\n",
    "    \n",
    "    condition1 = (int(torch.argmax(out)) != self.loss.neuron) and (self.loss.maximise == 0)\n",
    "    condition2 = (int(torch.argmax(out)) == self.loss.neuron) and (self.loss.maximise == 1)\n",
    "    if condition1 or condition2:\n",
    "        return x, losses, outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer description\n",
    "Create a file (markdown) possibly where all the arguments are explained and then write dwon the suggest value from the papers (if any) and the empirical one found in the datasets.\n",
    "\n",
    "\n",
    "**Example 1** \n",
    "\n",
    "**Name:**  Zero Stochastic Gradient Descent <br>\n",
    "**Class:** zeroOptim.zeroSGD <br>\n",
    "**Paper:** *Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints, High-Dimensionality and Saddle-Points* (rishnakumar Balasubramanian†1 and Saeed Ghadimi‡2) <br>\n",
    "\n",
    "\n",
    "**Description:**\n",
    "\n",
    "The zero-order Stochastic Gradient Descent its like the classical SGD but with the gradient G computed as follows:\n",
    "\n",
    "$$G_{v}^{k} \\equiv G_{v}(z_{k-1}, \\xi_{k}, u_{k}) = \\frac{1}{m_{k}} \\sum_{j=1}^{m_{k}} \\frac{F(z_{k-1} + vu_{k,j}, \\xi_{k,j}) - (z_{k-1}, \\xi_{k,j})}{v}u_{k,j}$$\n",
    "\n",
    "where: <br>\n",
    "$z_{k}$ is our optimization parameter <br>\n",
    "$\\xi_{k}$ is a sample of our distribution <br>\n",
    "$u_{k,j} \\sim N(0, I_{d})$ <br>\n",
    "$m_{k}$ is an input parameter <br>\n",
    "$v_{k}$ is the gaussian smoothing parameter\n",
    "\n",
    "**Args:**\n",
    "    \n",
    "    Name            Type                Description\n",
    "    x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "    v:              (float)             The gaussian smoothing\n",
    "    n_gradient:     (list)              Number of normal vector to generate at every step. Its (mk)\n",
    "    ak              (list)              Pseudo learning rate every step\n",
    "    epsilon:        (float)             The upper bound of the infinity norm\n",
    "    C:              (tuple)             The boundaries of the pixel. Default is (0, 1)\n",
    "    max_steps:      (int)               The maximum number of steps. Default is 100\n",
    "    verbose:        (int)               Display information or not. Default is 0\n",
    "    additional_out  (bool)              Return also all the x. Default is False\n",
    "    tqdm_disable    (bool)              Disable the tqdm bar. Default is False\n",
    "     \n",
    "**Suggested values:**\n",
    "\n",
    "$v = \\sqrt{\\frac{2B_{L_{\\sigma}}}{N(d+3)^3}}$, \n",
    "$\\alpha_{k} =\\frac{1}{\\sqrt{N}}$,\n",
    "$m_{k} = 2B_{L_{\\sigma}}(d + 5)N$,\n",
    "$\\forall k \\geq 1$\n",
    "\n",
    "where:<br>\n",
    "- *N* is the number of steps <br>\n",
    "- *d* is the dimension of *x* <br>\n",
    "- $B_{L_{\\sigma}} ≥ max\\bigg\\{\\sqrt{\\frac{B^2 + \\sigma^2}{L}}, 1\\bigg\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
