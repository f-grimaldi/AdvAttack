{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer description: InexactZSCG\n",
    "**Name:** Zero Stochastic Conditional Gradient with Inexact Updates<br>\n",
    "**Class:** zeroOptim.InexactZSCG <br>\n",
    "**Paper:** *Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints, High-Dimensionality and Saddle-Points* (rishnakumar Balasubramanian†1 and Saeed Ghadimi‡2) <br>\n",
    "\n",
    "\n",
    "**Description:** <br>\n",
    "The Zero-order Stochastic Conditional Gradient Descent with Inexact Updates is modified version of the classic *ZSCG* that implements the *Inexac Conditional Gradient*. At each iteration *k* try to minimize *F(x)* with these 2 main steps:\n",
    "\n",
    "    1. Estimate the gradient as follow:\n",
    "$$G_{v}^{k} \\equiv G_{v}(x_{k-1}, \\xi_{k}, u_{k}) = \\frac{1}{m_{k}} \\sum_{j=1}^{m_{k}} \\frac{F(x_{k-1} + vu_{k,j}, \\xi_{k,j}) - (x_{k-1}, \\xi_{k,j})}{v}u_{k,j}$$\n",
    "    2. Use ICG to compute the new x\n",
    "$$x_{k+1} = ICG(x_{k}, G_{v}^{k}, \\gamma_{k}, \\mu_{k}) $$\n",
    "\n",
    "\n",
    "    where ICG is described as follow:\n",
    "\n",
    "&emsp;&emsp;Input is ($x, g, \\gamma, \\mu$) <br>\n",
    "&emsp;&emsp;Set $\\hat{y}_{0} = x$, $t=0$ amd $n=0$ <br>\n",
    "&emsp;&emsp;*While* n = 0: <br>\n",
    "$$y_{t} = argmin_{u \\in \\chi}\\big\\{h_{\\gamma}(u) := \\langle g + \\gamma(\\hat{y}_{t-1} - x), u - \\hat{y}_{t-1}\\rangle \\big\\}$$\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;If $h_{\\gamma}(y_{t}) \\geq \\mu$, set $n = 1$<br>\n",
    "&emsp;&emsp;&emsp;&emsp;Else $\\hat{y}_{t} = \\frac{t-1}{t+1}\\hat{y}_{t-1} + \\frac{2}{t+1}y_{t}$<br>\n",
    "&emsp;&emsp;*end while*<br>\n",
    "&emsp;&emsp;Output $\\hat{y}_{t}$\n",
    "    \n",
    "where: <br>\n",
    "$x_{k}$ is our optimization parameter <br>\n",
    "$\\xi_{k}$ is a sample of our distribution <br>\n",
    "$u_{k,j} \\sim N(0, I_{d})$ <br>\n",
    "$m_{k}$ is the number of gaussian vector to generate <br>\n",
    "$v$ is the gaussian smoothing parameter <br>\n",
    "$\\gamma_{k}$ is the momentum inside ICG at time k <br>\n",
    "$\\mu_{k}$ is the stopping criterion of ICG at time k <br>\n",
    "\n",
    "**Args:**\n",
    "\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v               (float)             The gaussian smoothing\n",
    "        n_gradient      (list)              Number of normal vector to generate at every step\n",
    "        gamma_k         (list)              Momentum at every step inside ICG\n",
    "        mu_k            (list)              Stoppinc criterion at every step k inside ICG\n",
    "        max_t           (int)               The maximum number of iteration inside of ICG.\n",
    "        epsilon         (float)             The upper bound of norm\n",
    "        L_type          (int)               Either -1 for L_infinity or x for Lx. Default is -1\n",
    "        batch_size      (int)               Maximum parallelization during the gradient estimation. Default is -1 (=n_grad)\n",
    "        C               (tuple)             The boundaires of the pixel. Default is (0, 1)\n",
    "        max_steps       (int)               The maximum number of steps. Default is 100\n",
    "        verbose         (int)               Display information or not. Default is 0\n",
    "        additional_out  (bool)              Return also all the x. Default is False\n",
    "        tqdm_disable    (bool)              Disable the tqdm bar. Default is False                      \n",
    "     \n",
    "**Suggested values:** <br>\n",
    "$v = \\sqrt{\\frac{1}{2N(d+3)^3}}$, \n",
    "$\\gamma_{k} =2L$,\n",
    "$\\mu_{k} = \\frac{1}{4N}$\n",
    "$m_{k} = 6(d + 5)N$,\n",
    "$\\forall k \\geq 1$\n",
    "\n",
    "where:<br>\n",
    "- *N* is the number of steps <br>\n",
    "- *d* is the dimension of *x* <br>\n",
    "- *L* is the constant of the Lipschitz gradient of f\n",
    "\n",
    "**Empirical values:** <br>\n",
    "In case of MNIST we can set:<br>\n",
    "$N = 100$ and $d = 784$, so:<br>\n",
    "\n",
    "- $v = 3e-6$\n",
    "- $\\gamma_{k} = 2$\n",
    "- $\\mu_{k} = 0.0025$\n",
    "- $m_{k} = 473400$\n",
    "\n",
    "**N.B** <br>\n",
    "In reality it has been seen that the *ICG* cycle takes a lot of time to converge to the sopping criterion, but usually we have good results after less than an hundred iteration. So the algorithm works much more efficiently when we set a maximum *t* inside *ICG*. <br>\n",
    "Moreover, as the classic ZSCG, the number of function evaluation needed to have a good approximation of the gradient ($m_{k}$ in the paper, *n_gradient* in the *run* arguments) can be much less than the one indicated by the paper. Infact if the paper multiply the dimensions *d* by 6 and by *N* we found that this parameters doesn't depend by N and can be reduced even to *d*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The results are all taken with the torch random seed set as *42*. It has been seen that a good convergence time can be achivied with $\\gamma \\in [2, 4]$, *max_t* $\\in [50, 200]$ and $\\mu = 0.0025$.\n",
    "\n",
    "The maximum number of step has been set to 100.\n",
    "\n",
    "**N.B** <br>\n",
    "All the results are taken in the *google colab enviroment* using the available GPU *Tesla K80*. \n",
    "\n",
    "\n",
    "**1. MNIST**\n",
    "    \n",
    "    1.a) Untarget\n",
    "        \n",
    "         Check results at this values of epsilon (0.25, 0.20, 0.15, 0.10, 0.05) for infinity norm\n",
    "         \n",
    "                 \n",
    "                 \n",
    "    1.b) Target\n",
    "        \n",
    "         Check results at this values of epsilon (0.50, 0.40, 0.30, 0.20, 0.10) for infinity norm\n",
    "    \n",
    "       \n",
    "**2 Cifar10**\n",
    "\n",
    "\n",
    "    1.a) Untarget\n",
    "        \n",
    "         Check results at this values of epsilon (0.02, 0.01, 0.005) for infinity norm\n",
    "         \n",
    "                 \n",
    "                 \n",
    "    1.b) Target\n",
    "        \n",
    "         Check results at this value of epsilon (0.02, 0.01, 0.005) for infinity norm\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
