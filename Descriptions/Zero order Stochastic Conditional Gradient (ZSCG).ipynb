{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer description: ZSCG\n",
    "**Name:**  Zero Stochastic Conditional Gradient<br>\n",
    "**Class:** zeroOptim.ClassicZSCG <br>\n",
    "**Paper:** *Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints, High-Dimensionality and Saddle-Points* (rishnakumar Balasubramanian†1 and Saeed Ghadimi‡2) <br>\n",
    "\n",
    "\n",
    "**Description:** <br>\n",
    "The Zero-order Stochastic Conditional Gradient Descent at each iteration *k* try to minimize *F(z)* with these 3 main steps:\n",
    "\n",
    "    1. Estimate the gradient as follow:\n",
    "$$G_{v}^{k} \\equiv G_{v}(z_{k-1}, \\xi_{k}, u_{k}) = \\frac{1}{m_{k}} \\sum_{j=1}^{m_{k}} \\frac{F(z_{k-1} + vu_{k,j}, \\xi_{k,j}) - (z_{k-1}, \\xi_{k,j})}{v}u_{k,j}$$\n",
    "    2. Solve this linear programming problem\n",
    "$$x_{k} = argmin_{u\\in\\chi}\\langle G_{v}^{k}, u\\rangle$$ \n",
    "    3. Update z\n",
    "$$z_{k+1} = (1-\\alpha_{k})z_{k} + \\alpha_{k} x_{k}$$ \n",
    "\n",
    "where: <br>\n",
    "$z_{k}$ is our optimization parameter <br>\n",
    "$\\xi_{k}$ is a sample of our distribution <br>\n",
    "$u_{k,j} \\sim N(0, I_{d})$ <br>\n",
    "$m_{k}$ is the number of gaussian vector to generate <br>\n",
    "$\\alpha_{k}$ is the momentum at time k <br>\n",
    "$v$ is the gaussian smoothing parameter <br>\n",
    "\n",
    "**Args:**\n",
    "\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v               (float)             The gaussian smoothing\n",
    "        n_gradient      (list)              Number of normal vector to generate at every step\n",
    "        ak              (list)              Momentum  every step\n",
    "        epsilon         (float)             The upper bound of norm\n",
    "        L_type          (int)               Either -1 for L_infinity or x for Lx. Default is -1\n",
    "        batch_size      (int)               Maximum parallelization during the gradient estimation. Default is -1 (=n_grad)\n",
    "        C               (tuple)             The boundaires of the pixel. Default is (0, 1)\n",
    "        max_steps       (int)               The maximum number of steps. Default is 100\n",
    "        verbose         (int)               Display information or not. Default is 0\n",
    "        additional_out  (bool)              Return also all the x. Default is False\n",
    "        tqdm_disable    (bool)              Disable the tqdm bar. Default is False\n",
    "\n",
    "\n",
    "     \n",
    "     \n",
    "**Suggested values:** <br>\n",
    "$v = \\sqrt{\\frac{2B_{L_{\\sigma}}}{N(d+3)^3}}$, \n",
    "$\\alpha_{k} =\\frac{1}{\\sqrt{N}}$,\n",
    "$m_{k} = 2B_{L_{\\sigma}}(d + 5)N$,\n",
    "$\\forall k \\geq 1$\n",
    "\n",
    "where:<br>\n",
    "- *N* is the number of steps <br>\n",
    "- *d* is the dimension of *x* <br>\n",
    "- $\\sigma$ is the Strong Convexity coefficient\n",
    "- $B \\geq ||f(x)||, \\forall x \\in \\chi$\n",
    "- $B_{L_{\\sigma}} ≥ max\\bigg\\{\\sqrt{\\frac{B^2 + \\sigma^2}{L}}, 1\\bigg\\}$\n",
    "\n",
    "**Empirical values:** <br>\n",
    "In case of MNIST we can set:<br>\n",
    "$N = 100$, $B_{L_{\\sigma}} = 1$ and we have a image 28 * 28 ($d = 784$), so:<br>\n",
    "- $v = 10e-6$\n",
    "- $\\alpha_{k} = 0.1$\n",
    "- $m_{k} = 78900$\n",
    "\n",
    "**N.B** <br>\n",
    "In reality it seems that $\\alpha_{k}$ could be set much higher (e.g. 0.3)  and $m_{k}$ could be set much lower (e.g. 600) and doesn't need to be dependent of the number of steps *N*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ClassicZSCG(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Name            Type                Description\n",
    "    model:          (nn.Module)         The model to use to get the output\n",
    "    loss:           (nn.Module)         The loss to minimize\n",
    "    device:\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss, device=torch.device('cuda')):\n",
    "        self.device = device\n",
    "        self.loss = loss\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    \"\"\"\n",
    "    Perform an attack against the model given an input and the required run params\n",
    "    \"\"\"\n",
    "    def run(self, x, v, n_gradient, ak , epsilon, L_type=-1, batch_size = -1, C = (0, 1), \n",
    "            max_steps=100, verbose=0, additional_out=False, tqdm_disabled=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v               (float)             The gaussian smoothing\n",
    "        n_gradient      (list)              Number of normal vector to generate at every step\n",
    "        ak              (list)              Momentum  every step\n",
    "        epsilon         (float)             The upper bound of norm\n",
    "        L_type          (int)               Either -1 for L_infinity or x for Lx. Default is -1\n",
    "        batch_size      (int)               Maximum parallelization during the gradient estimation. Default is -1 (=n_grad)\n",
    "        C               (tuple)             The boundaires of the pixel. Default is (0, 1)\n",
    "        max_steps       (int)               The maximum number of steps. Default is 100\n",
    "        verbose         (int)               Display information or not. Default is 0\n",
    "        additional_out  (bool)              Return also all the x. Default is False\n",
    "        tqdm_disable    (bool)              Disable the tqdm bar. Default is False\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        # 1. Init class attributes\n",
    "        self.x_original = x.clone()\n",
    "        self.dim = x.shape\n",
    "        self.total_dim = torch.prod(torch.tensor(x.shape))\n",
    "        self.epsilon = epsilon\n",
    "        self.L_type = L_type\n",
    "        self.C = C\n",
    "        self.batch = batch_size\n",
    "        \n",
    "\n",
    "        # 2. Init list of results\n",
    "        losses, outs = [], []\n",
    "        x_list = []\n",
    "\n",
    "        # 3. Main optimization cycle\n",
    "        for ep in tqdm(range(max_steps), disable=tqdm_disabled):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"---------------\")\n",
    "                print(\"Step number: {}\".format(ep))\n",
    "            \n",
    "            # 3.1 Call the step\n",
    "            x, gk = self.step(x, v, ak[ep], n_gradient[ep], verbose)\n",
    "            x = x.reshape(self.dim[0], self.dim[1], self.dim[2]).detach()\n",
    "            \n",
    "            # 3.2 Compute loss\n",
    "            out = self.model(x.view(1, self.dim[0], self.dim[1], self.dim[2]))\n",
    "            loss = self.loss(out)\n",
    "            \n",
    "            # 3.3 Save results\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            outs.append(out.detach().cpu()[0, self.loss.neuron].item())\n",
    "            if additional_out:\n",
    "                x_list.append(x.cpu())\n",
    "            \n",
    "            # 3.4 Display current info\n",
    "            if verbose:\n",
    "                print(\"Loss:        {}\".format(losses[-1]))\n",
    "                print(\"Output:      {}\".format(outs[-1]))\n",
    "            \n",
    "            # 3.5 Check Stopping criterions\n",
    "            condition1 = (int(torch.argmax(out)) != self.loss.neuron) and (self.loss.maximise == 0)\n",
    "            condition2 = (int(torch.argmax(out)) == self.loss.neuron) and (self.loss.maximise == 1)\n",
    "            if condition1 or condition2:\n",
    "                break\n",
    "\n",
    "        if additional_out:\n",
    "            return x, losses, outs, x_list\n",
    "        return  x, losses, outs\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Do an optimization step\n",
    "    \"\"\"\n",
    "    def step(self, x, v, ak, mk, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v:              (float)             The gaussian smoothing\n",
    "        ak:             (float)             The weight avarage in the updating phase. (1 means take only the new x, 0 means no update)\n",
    "        mk:             (int)               The number of Gaussian Random Vector to generate\n",
    "        verbose:        (bool)              Display information or not. Default is 0\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the approximated gradient\n",
    "        g = self.compute_Gk(x, v, mk, verbose)\n",
    "        \n",
    "        # Call the inexact conditional gradient\n",
    "        x_g = self.compute_CG(g, verbose).reshape(x.shape[0], x.shape[1], x.shape[2])\n",
    "        x_new = (1-ak)*x + ak*x_g\n",
    "\n",
    "        if verbose > 1:\n",
    "            print(\"\\nINSIDE STEP\")\n",
    "            print(\"Gradient has shape: {}\".format(g.shape))\n",
    "            print(\"Gradient is:\\n{}\".format(g))\n",
    "            print(\"x_new has shape: {}\".format(x_new.shape))\n",
    "            print(\"x_new is:\\n{}\".format(x_new))\n",
    "\n",
    "        return x_new.detach(), g.detach()\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare parallelization for the gradient estimation\n",
    "    \"\"\"\n",
    "    def get_parallel(self, bs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        bs              (int)               The maximum bacth size\n",
    "        \"\"\"\n",
    "        uk     = torch.empty(bs, self.total_dim).normal_(mean=0, std=1).to(self.device) # Dim (bs, channel*width*height)\n",
    "        img_u  = uk.reshape(bs, self.dim[0], self.dim[1], self.dim[2])                  # Dim (bs, channel, width, height)\n",
    "        img_x  = x.expand(bs, self.dim[0], self.dim[1], self.dim[2])                    # Dim (bs, channel, width, height)\n",
    "        m_x    = (img_x + v*img_u)\n",
    "        \n",
    "        return m_x, uk\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Gv(x(k-1), chi(k-1), u(k)) in order to compute an approximation of the gradient of f(x(k-1), chi(k-1))\n",
    "    \"\"\"\n",
    "    def compute_Gk(self, x, v, mk, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v:              (float)             The gaussian smoothing\n",
    "        mk:             (int)               The number of Gaussian Random Vector to generate\n",
    "        verbose:        (bool)              Display information or not. Default is 0\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Get objective functions\n",
    "\n",
    "        if self.batch == -1:\n",
    "            \n",
    "            # 1.a Compute standard Loss\n",
    "            standard_loss = self.loss(self.model(x.view(1, *list(self.dim)))) \n",
    "            \n",
    "                                                               \n",
    "            # 1.b Compute gaussian loss\n",
    "            uk, m_x = self.get_parallel(mk)\n",
    "            gaussian_loss = self.loss(self.model(m_x))\n",
    "            \n",
    "            if verbose > 1:\n",
    "                print('\\nINSIDE GRADIENT')\n",
    "                print('The Gaussian vector uk has shape:{}'.format(uk.shape))\n",
    "                print('The input x has shape:\\t\\t{}'.format(x.shape))\n",
    "                print('The input x + vu has shape:\\t{}'.format(m_x.shape))\n",
    "\n",
    "\n",
    "            # 1.c Compute Gv(x(k-1), chi(k-1), u(k))\n",
    "            fv = ((gaussian_loss - standard_loss.expand(uk.shape[0]))/v).view(-1, 1)        # Dim (mk, 1)\n",
    "            G = fv * uk                                                                     # Dim (mk, channel*width*height)\n",
    "\n",
    "            return torch.mean(G, axis=0).detach()\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # 1.a Compute standard loss\n",
    "            standard_loss = self.loss(self.model(x.view(1, *list(self.dim))))                   # Dim (1)\n",
    "            G_tot = torch.zeros(mk//self.batch, self.total_dim).to(self.device)                 # Dim (n_batches, hannel*width*height)\n",
    "\n",
    "            #1.b Compute Gradient\n",
    "            for n in range(mk//self.batch):\n",
    "                from_, to_ = n*self.batch, (n+1)*self.batch\n",
    "\n",
    "                # 1.b Create batch x(k-1) + v*u(k-1)\n",
    "                uk, mk = self.get_parallel(self.batch)\n",
    "\n",
    "                # 1.c Compute\n",
    "                tmp_gaussian_loss = self.loss(self.model(m_x)).detach()                                 # Dim(bs)\n",
    "                \n",
    "                if verbose > 1:\n",
    "                    print('\\nINSIDE GRADIENT')\n",
    "                    print('The Gaussian vector uk has shape:{}'.format(uk.shape))\n",
    "                    print('The input x has shape:\\t\\t{}'.format(x.shape))\n",
    "                    print('The input x + vu has shape:\\t{}'.format(m_x.shape))\n",
    "                \n",
    "                # 1.d Compute Gradient\n",
    "                fv = ((tmp_gaussian_loss - standard_loss.expand(uk.shape[0]))/v).view(-1, 1)            # Dim (bs, 1)\n",
    "                G = fv * uk                                                                             # Dim (bs, channel*width*height)\n",
    "\n",
    "                if verbose > 1:\n",
    "                    print('Gaussian cycle loss has shape:\\t{}'.format(tmp_gaussian_loss.shape))\n",
    "                    print('Function approx has shape:\\t{}'.format(fv.shape))\n",
    "                    print('Gradient has shape:\\t\\t{}'.format(G.shape))\n",
    "\n",
    "                G_tot[n] = torch.mean(G, axis=0).detach()\n",
    "\n",
    "        return torch.mean(G_tot, axis=0).detach()\n",
    "\n",
    "\n",
    "    def compute_CG(self, g, verbose):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name         Type                Description\n",
    "        g:           (torch.tensor)      The approximated gradient. Should be a 1D tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Infinity norm\n",
    "        if self.L_type == -1:\n",
    "            x_new = self.x_original.view(-1) - self.epsilon*torch.sign(g)\n",
    "            \n",
    "        # L1 norm\n",
    "        elif self.L_type == 1:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        elif self.L_type == 2:\n",
    "            x_new = self.x_original.view(-1) - (self.epsilon*g)/torch.norm(g, 2)\n",
    "            \n",
    "        # Generic Lp norm (1 < p < +inf)\n",
    "        else:\n",
    "            p = self.L_type\n",
    "            gp = torch.abs(g)**(1/p-1)\n",
    "            h = torch.sign(g) * (gp) / torch.norm(gp, p)\n",
    "            x_new = self.x_original.view(-1) - self.epsilon*h\n",
    "\n",
    "        # Enforce pixel boundaries\n",
    "        x_new[x_new < self.C[0]] = self.C[0]\n",
    "        x_new[x_new > self.C[1]] = self.C[1]\n",
    "\n",
    "        if verbose > 1:\n",
    "            print('\\nINSIDE CG')\n",
    "            print('Epsilon * Sign(g) is {}'.format(self.epsilon*torch.sign(g)))\n",
    "            print('Unchecked new x is: {}'.format(self.x_original.view(-1) - self.epsilon*torch.sign(g)))\n",
    "            print('The CG gradient is:\\n{}'.format(g))\n",
    "            print('The new x is:\\n {}'.format(x_new))\n",
    "\n",
    "        return x_new.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InexactZSCG(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Name            Type                Description\n",
    "    model:          (nn.Module)         The model to use to get the output\n",
    "    loss:           (nn.Module)         The loss to minimize\n",
    "    device:\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss, device=torch.device('cuda')):\n",
    "        self.device = device\n",
    "        self.loss = loss\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform an attack against the model given an input and the required run params\n",
    "    \"\"\"\n",
    "    def run(self, x, v, n_gradient, gamma_k, mu_k, epsilon, L_type = -1, batch_size = -1, C = (0, 1), \n",
    "            max_steps=100, verbose=0, additional_out=False, tqdm_disabled=False, max_t=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v               (float)             The gaussian smoothing\n",
    "        n_gradient      (list)              Number of normal vector to generate at every step\n",
    "        gamma_k         (list)              Momentum at every step inside ICG\n",
    "        mu_k            (list)              Stoppinc criterion at every step k inside ICG\n",
    "        max_t           (int)               The maximum number of iteration inside of ICG.\n",
    "        epsilon         (float)             The upper bound of norm\n",
    "        L_type          (int)               Either -1 for L_infinity or x for Lx. Default is -1\n",
    "        batch_size      (int)               Maximum parallelization during the gradient estimation. Default is -1 (=n_grad)\n",
    "        C               (tuple)             The boundaires of the pixel. Default is (0, 1)\n",
    "        max_steps       (int)               The maximum number of steps. Default is 100\n",
    "        verbose         (int)               Display information or not. Default is 0\n",
    "        additional_out  (bool)              Return also all the x. Default is False\n",
    "        tqdm_disable    (bool)              Disable the tqdm bar. Default is False    \n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        # 1. Init class attributes\n",
    "        self.x_original = x.clone()\n",
    "        self.dim = x.shape\n",
    "        self.total_dim = torch.prod(torch.tensor(x.shape))\n",
    "        self.epsilon = epsilon\n",
    "        self.L_type = L_type\n",
    "        self.C = C\n",
    "        self.batch = batch_size\n",
    "        self.max_t = max_t\n",
    "\n",
    "        # 2. Init list of results\n",
    "        losses, outs = [], [] \n",
    "        x_list = []\n",
    "\n",
    "        # 3. Main optimization cycle\n",
    "        for ep in tqdm(range(max_steps), disable=tqdm_disabled):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"---------------\")\n",
    "                print(\"Step number: {}\".format(ep))\n",
    "            \n",
    "            # 3.1 Call the step\n",
    "            x, gk = self.step(x, v, gamma_k[ep], mu_k[ep], n_gradient[ep], verbose)\n",
    "            x = x.reshape(self.dim[0], self.dim[1], self.dim[2]).detach()\n",
    "            \n",
    "            # 3.2 Compute loss\n",
    "            out = self.model(x.view(1, self.dim[0], self.dim[1], self.dim[2]))\n",
    "            loss = self.loss(out).view(-1, 1)\n",
    "            \n",
    "            # 3.3 Save results\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            outs.append(out.detach().cpu()[0, self.loss.neuron].item())\n",
    "            if additional_out:\n",
    "                x_list.append(x.cpu())\n",
    "            \n",
    "            # 3.4 Display current info\n",
    "            if verbose:\n",
    "                print(\"Loss:        {}\".format(losses[-1]))\n",
    "                print(\"Output:      {}\".format(outs[-1]))\n",
    "                \n",
    "            # 3.5 Check Stopping criterion\n",
    "            condition1 = (int(torch.argmax(out)) != self.loss.neuron) and (self.loss.maximise == 0)\n",
    "            condition2 = (int(torch.argmax(out)) == self.loss.neuron) and (self.loss.maximise == 1)\n",
    "            if condition1 or condition2:\n",
    "                break\n",
    "\n",
    "        if additional_out:\n",
    "            return x, losses, outs, input_list\n",
    "        return  x, losses, outs\n",
    "\n",
    "    \"\"\"\n",
    "    Do an optimization step\n",
    "    \"\"\"\n",
    "    def step(self, x, v, gamma, mu, mk, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v:              (float)             The gaussian smoothing\n",
    "        gamma:          (float)             The update parameters of g\n",
    "        mu:             (float)             The stopping criterion\n",
    "        mk:             (int)               The number of Gaussian Random Vector to generate\n",
    "        verbose:        (bool)              Display information or not. Default is 0\n",
    "        \"\"\"\n",
    "        # Compute the approximated gradient\n",
    "        g = self.compute_Gk(x, v, mk, verbose)\n",
    "        # Call the inexact conditional gradient\n",
    "        x_new = self.compute_ICG(x, g, gamma, mu, verbose).reshape(x.shape[0], x.shape[1], x.shape[2])\n",
    "    \n",
    "        if verbose > 1:\n",
    "            print(\"\\nINSIDE STEP\")\n",
    "            print(\"Gradient has shape: {}\".format(g.shape))\n",
    "            print(\"Gradient is:\\n{}\".format(g))\n",
    "            print(\"x_new has shape: {}\".format(x_new.shape))\n",
    "            print(\"x_new is:\\n{}\".format(x_new))\n",
    "\n",
    "        return x_new.detach(), g.detach()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare parallelization for the gradient estimation\n",
    "    \"\"\"\n",
    "    def get_parallel(self, x, bs, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x               (torch.tensor)      The current variable\n",
    "        bs              (int)               The maximum bacth size\n",
    "        v               (int)               The Gaussian smoothing\n",
    "        \"\"\"\n",
    "        uk     = torch.empty(bs, self.total_dim).normal_(mean=0, std=1).to(self.device) # Dim (bs, channel*width*height)\n",
    "        img_u  = uk.reshape(bs, self.dim[0], self.dim[1], self.dim[2])                  # Dim (bs, channel, width, height)\n",
    "        img_x  = x.expand(bs, self.dim[0], self.dim[1], self.dim[2])                    # Dim (bs, channel, width, height)\n",
    "        m_x    = (img_x + v*img_u)\n",
    "\n",
    "        return m_x, uk\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Gv(x(k-1), chi(k-1), u(k)) in order to compute an approximation of the gradient of f(x(k-1), chi(k-1))\n",
    "    \"\"\"\n",
    "    def compute_Gk(self, x, v, mk, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        v:              (float)             The gaussian smoothing\n",
    "        mk:             (int)               The number of Gaussian Random Vector to generate\n",
    "        verbose:        (bool)              Display information or not. Default is 0\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Get objective functions\n",
    "        # CASE BATCH_SIZE == N_GRADIENT\n",
    "        if self.batch == -1:\n",
    "\n",
    "            # 1.a Compute standard Loss\n",
    "            standard_loss = self.loss(self.model(x.view(1, *list(self.dim))))\n",
    "\n",
    "            # 1.b Compute gaussian loss\n",
    "            m_x, uk = self.get_parallel(x, mk, v)\n",
    "            gaussian_loss = self.loss(self.model(m_x))\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('\\nINSIDE GRADIENT')\n",
    "                print('The Gaussian vector uk has shape:{}'.format(uk.shape))\n",
    "                print('The input x has shape:\\t\\t{}'.format(x.shape))\n",
    "                print('The input x + vu has shape:\\t{}'.format(m_x.shape))\n",
    "\n",
    "            # 1.c Compute Gv(x(k-1), chi(k-1), u(k))\n",
    "            fv = ((gaussian_loss - standard_loss.expand(uk.shape[0]))/v).view(-1, 1)        # Dim (mk, 1)\n",
    "            G = fv * uk                                                                     # Dim (mk, channel*width*height)\n",
    "\n",
    "            return torch.mean(G, axis=0).detach()\n",
    "\n",
    "        # CASE BATCH_SIZE < N_GRADIENT\n",
    "        else:\n",
    "\n",
    "            # 1.a Compute standard loss\n",
    "            standard_loss = self.loss(self.model(x.view(1, *list(self.dim))))                   # Dim (1)\n",
    "            G_tot = torch.zeros(mk//self.batch, self.total_dim).to(self.device)                 # Dim (n_batches, hannel*width*height)\n",
    "\n",
    "            #1.b Compute Gradient\n",
    "            for n in range(mk//self.batch):\n",
    "                from_, to_ = n*self.batch, (n+1)*self.batch\n",
    "\n",
    "                # 1.b Create batch x(k-1) + v*u(k-1)\n",
    "                m_x, uk = self.get_parallel(x, self.batch, v)\n",
    "\n",
    "                # 1.c Compute\n",
    "                tmp_gaussian_loss = self.loss(self.model(m_x)).detach()                                 # Dim(bs)\n",
    "\n",
    "                if verbose > 1:\n",
    "                    print('\\nINSIDE GRADIENT')\n",
    "                    print('The Gaussian vector uk has shape:{}'.format(uk.shape))\n",
    "                    print('The input x has shape:\\t\\t{}'.format(x.shape))\n",
    "                    print('The input x + vu has shape:\\t{}'.format(m_x.shape))\n",
    "\n",
    "                # 1.d Compute Gradient\n",
    "                fv = ((tmp_gaussian_loss - standard_loss.expand(uk.shape[0]))/v).view(-1, 1)            # Dim (bs, 1)\n",
    "                G = fv * uk                                                                             # Dim (bs, channel*width*height)\n",
    "\n",
    "                if verbose > 1:\n",
    "                    print('Gaussian cycle loss has shape:\\t{}'.format(tmp_gaussian_loss.shape))\n",
    "                    print('Function approx has shape:\\t{}'.format(fv.shape))\n",
    "                    print('Gradient has shape:\\t\\t{}'.format(G.shape))\n",
    "\n",
    "                G_tot[n] = torch.mean(G, axis=0).detach()\n",
    "\n",
    "        return torch.mean(G_tot, axis=0).detach()\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Inexact Condtion Gradient (Algorithm 3 of source article)\n",
    "    \"\"\"\n",
    "    def compute_ICG(self, x, g, gamma, mu, verbose):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        Name            Type                Description\n",
    "        x:              (torch.tensor)      The variable of our optimization problem. Should be a 3D tensor (img)\n",
    "        g:              (torch.tensor)      The approximated gradient. Should be a 1D tensor\n",
    "        gamma:          (float)             The update parameters of g\n",
    "        mu:             (float)             The stopping criterion\n",
    "        \"\"\"\n",
    "        # 1. Init variables\n",
    "        y_old = x.view(-1).clone() # dim = (n_channel * width * height)\n",
    "        u = torch.rand(self.total_dim).to(self.device)*(self.max.view(-1) - self.min.view(-1)) + self.min.view(-1)\n",
    "        t = 1\n",
    "        k = 0\n",
    "\n",
    "        # 2. Main cycle\n",
    "        while(k==0):\n",
    "            \n",
    "            # 2.1 Compute gradient\n",
    "            grad = g + gamma*(y_old - x.view(-1))\n",
    "            \n",
    "            # 2.2 Perform LMO\n",
    "            # Infinity norm\n",
    "            if self.L_type == -1:\n",
    "                x_new = self.x_original.view(-1) - self.epsilon*torch.sign(g)\n",
    "            # L1 norm\n",
    "            elif self.L_type == 1:\n",
    "                raise NotImplementedError\n",
    "            elif self.L_type == 2:\n",
    "                x_new = self.x_original.view(-1) - (self.epsilon*g)/torch.norm(g, 2)\n",
    "            # Generic Lp norm (1 < p < +inf)\n",
    "            else:\n",
    "                p = self.L_type\n",
    "                gp = torch.abs(g)**(1/p-1)\n",
    "                h = torch.sign(g) * (gp) / torch.norm(gp, p)\n",
    "                x_new = self.x_original.view(-1) - self.epsilon*h\n",
    "                \n",
    "            # 2.3 Compute new function value\n",
    "            h = torch.dot(grad, y_new - y_old)\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('\\nINSIDE ICG')\n",
    "                print('Time t = {}'.format(t))\n",
    "                print('The ICG gradient is:\\n{}'.format(grad))\n",
    "                print('The new y is:\\n {}'.format(y_new))\n",
    "                print('The function h(y_new) is {}'.format(h))\n",
    "                print('Mu is: {}'.format(mu))\n",
    "                \n",
    "            # 2.4 Check conditions\n",
    "            if h >= -mu or t > self.max_t:\n",
    "                k = 1\n",
    "            else:\n",
    "                y_old = (t-1)/(t+1)*y_old + 2/(t+1)*y_new\n",
    "                t += 1\n",
    "\n",
    "        return self.project_boundaries(y_old.detach())\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Check the boundaries of our constraint optimization problem\n",
    "    \"\"\"\n",
    "    def project_boundaries(self, x):\n",
    "        x[x > self.C[1]] = self.C[1]\n",
    "        x[x < self.C[0]] = self.C[0]\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
